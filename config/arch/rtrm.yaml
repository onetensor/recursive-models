name: recursive_reasoning.rtrm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: rtrm_losses@ACTLossHead
  loss_type: stablemax_cross_entropy
  stickiness_weight: 0.0
  stickiness_residual_type: logits_l2
  stickiness_horizon: 1
  stickiness_gamma: 1.0

halt_exploration_prob: 0.1
halt_max_steps: 16

H_cycles: 3
L_cycles: 6

H_layers: 0
L_layers: 2

hidden_size: 512
num_heads: 8  # min(2, hidden_size // 64)
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope
forward_dtype: bfloat16

mlp_t: False # use mlp on L instead of transformer
puzzle_emb_len: 16 # if non-zero, its specified to this value
no_ACT_continue: True # No continue ACT loss, only use the sigmoid of the halt which makes much more sense

# Residual computation / logging
residual_enabled: False
residual_type: logits_l2 # logits_l2 | prob_kl_sym
residual_temp: 1.0
residual_trace_enabled: False

# Residual-based halting
halt_residual_enabled: False
halt_residual_stat: max # mean | max
halt_residual_tau: 1e-3
halt_residual_patience: 2
halt_residual_min_steps: 2

# Confidence gate
halt_confidence_min: 0.0
halt_confidence_stat: min # mean | min
halt_confidence_temp: 1.0

# Explicit residual update / damping
update_damping_enabled: False
update_damping_alpha_zL: 1.0
update_damping_alpha_zH: 1.0
